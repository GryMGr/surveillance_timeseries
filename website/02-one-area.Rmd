# Panel Data - One Area

## Aim

We are given a dataset containing daily counts of diseases from one geographical area. We want to identify:

1. Does seasonality exist?
2. If seasonality exists, when are the high/low seasons?
3. Is there a general yearly trend (i.e. increasing or decreasing from year to year?)
4. Is daily rainfall associated with the number of cases?
5. When are there outbreaks?

```{r warning=FALSE}
library(data.table)
library(ggplot2)
set.seed(4)

AMPLITUDE <- 1.5
SEASONAL_HORIZONTAL_SHIFT <- 20

d <- data.table(date = seq.Date(
  from = as.Date("2000-01-01"),
  to = as.Date("2018-12-31"),
  by = 1
))
d[, date := as.Date(date, origin = "1970-01-1")]
d[, year := as.numeric(format.Date(date, "%G"))]
d[, week := as.numeric(format.Date(date, "%V"))]
d[, month := as.numeric(format.Date(date, "%m"))]
d[, yearMinus2000 := year - 2000]
d[, dailyrainfall := runif(.N, min = 0, max = 10)]

d[, dayOfYear := as.numeric(format.Date(date, "%j"))]
d[, seasonalEffect := sin(2 * pi * (dayOfYear - SEASONAL_HORIZONTAL_SHIFT) / 365)]
d[, mu := exp(0.1 + yearMinus2000 * 0.1 + seasonalEffect * AMPLITUDE)]
d[, y := rpois(.N, mu)]
```

## Data

Here we show the true data, and note that there is an increasing annual trend (the data gets higher as time goes on) and there is a seasonal pattern (one peak/trough per year)
```{r}
q <- ggplot(d, aes(x = date, y = y))
q <- q + geom_line(lwd = 0.25)
q <- q + scale_x_date("Time")
q <- q + scale_y_continuous("Cases")
q
```

We split out the data for a few years and see a clear seasonal trend:

```{r message=FALSE, warning=FALSE}
q <- ggplot(d[year %in% c(2005:2010)], aes(x = dayOfYear, y = y))
q <- q + facet_wrap(~year)
q <- q + geom_point()
q <- q + stat_smooth(colour = "red")
q <- q + scale_x_continuous("Day of year")
q <- q + scale_y_continuous("Cases")
q
```

## Model With Non-Parametric Seasonality

If we want to investigate the seasonality of our data, and identify when are the peaks and troughs, we can use non-parametric approaches. They are flexible and easy to implement, but they can lack power and be hard to interpret:

- Create a categorical variable for the seasons (e.g. `spring`, `summer`, `autumn`, `winter`) and include this in the regression model
- Create a categorical variable for the months (e.g. `Jan`, `Feb`, ..., `Dec`) and include this in the regression model

```{r}
nfit0 <- glm(y ~ yearMinus2000 + dailyrainfall, data = d, family = poisson())
nfit1 <- glm(y ~ yearMinus2000 + dailyrainfall + as.factor(month), data = d, family = poisson())
```

### Seasonality

We can test the `month` categorical variable using a likelihood ratio test:

```{r}
lmtest::lrtest(nfit0, nfit1)
```

**Question 1:** Does seasonality exist?

And then we can look at the output of our regression:

```{r}
summary(nfit1)
```

*NOTE:* See that this is basically the same as a normal regression.

**Question 2:** If seasonality exists, when are the high/low seasons?

### Yearly trend

**Question 3:** Is there a general yearly trend (i.e. increasing or decreasing from year to year?)

### Association With Rainfall

**Question 4:** Is daily rainfall associated with the number of cases?

### Outbreaks

If we want to identify outbreaks, then we need to use the standard prediction interval formula:

$$
95\% \text{ prediction interval} = \text{sample average} \pm 1.96 \times \text{sample standard deviation} \sqrt{ 1 + 1 / n}
$$
This allows us to identify what the expected thresholds are:

```{r}
pred <- predict(nfit1, type = "response", se.fit = T, newdata = d)
d[, threshold0 := pred$fit]
d[, threshold2 := sykdomspuls::FarringtonThreshold(pred, phi = 1, z = 2, skewness.transform = "2/3")]
```

**Question 5:** When are there outbreaks?

```{r}
q <- ggplot(d[year > 2015], aes(x = date, y = y))
q <- q + geom_ribbon(mapping = aes(ymin = -Inf, ymax = threshold2), fill = "blue", alpha = 0.5)
q <- q + geom_ribbon(mapping = aes(ymin = threshold2, ymax = Inf), fill = "red", alpha = 0.5)
q <- q + geom_line(lwd = 0.25)
q <- q + geom_point(data = d[year > 2015 & y > threshold2], colour = "black", size = 2.5)
q <- q + geom_point(data = d[year > 2015 & y > threshold2], colour = "red", size = 1.5)
q <- q + scale_x_date("Time")
q <- q + scale_y_continuous("Cases")
q
```

## Model With Parametric Seasonality 

Parametric approaches are more powerful but require more effort:

- Identify the periodicity of the seasonality (how many days between peaks?)
- Using trigonometry, transform `day of year` into variables that appropriately model the observed periodicity
- Obtain coefficient estimates
- Back-transform these estimates into human-understandable values (day of peak, day of trough)

*NOTE:* You don't always have to investigate seasonality! It depends entirely on what the purpose of your analysis is!

### Seasonality

The Lomb-Scargle Periodogram shows a clear seasonality with a period of 365 days.

```{r}
# R CODE
lomb::lsp(d$y, from = 100, to = 500, ofac = 1, type = "period")
```

We then generate two new variables `cos365` and `sin365` and perform a likelihood ratio test to see if they are significant or not. This is done with two simple poisson regressions.

```{r}
# R CODE
d[, cos365 := cos(dayOfYear * 2 * pi / 365)]
d[, sin365 := sin(dayOfYear * 2 * pi / 365)]

pfit0 <- glm(y ~ yearMinus2000 + dailyrainfall, data = d, family = poisson())
pfit1 <- glm(y ~ yearMinus2000 + dailyrainfall + sin365 + cos365, data = d, family = poisson())
```

We can test the seasonality using a likelihood ratio test (which we already strongly suspected due to the periodogram):

```{r}
lmtest::lrtest(pfit0, pfit1)
```

**Question 1:** Does seasonality exist?

And then we can look at the output of our regression:

```{r}
summary(pfit1)
```

We also see that the (significant!) coefficient for `year` is `0.1` which means that for each additional year, the outcome increases by `exp(0.1)=1.11`. We also see that the coefficient for `dailyrainfall` was not significant, which means that we did not find a significant association between the outcome and `dailyrainfall`.

*NOTE:* See that this is basically the same as a normal regression.

Through the likelihood ratio test we saw a clear significant seasonal effect. We can now use trigonometry to back-calculate the amplitude and location of peak/troughs from the `cos365` and `sin365` estimates:

```{r}
RAWmisc::TransformCosSinToAmplitudePeakTrough
```

```{r}
retval <- RAWmisc::TransformCosSinToAmplitudePeakTrough(
  cos_b = -0.512912, # cos coefficient
  sin_b = 1.428417 # sin coefficient
)

print(sprintf("amplitude is estimated as %s", round(retval$amplitude, 2)))
print(sprintf("peak is estimated as %s", round(retval$peak)))
print(sprintf("trough is estimated as %s", round(retval$trough)))

print(sprintf("true amplitude is %s", round(AMPLITUDE, 2)))
print(sprintf("true peak is %s", round(365 / 4 + SEASONAL_HORIZONTAL_SHIFT)))
print(sprintf("true trough is %s", round(3 * 365 / 4 + SEASONAL_HORIZONTAL_SHIFT)))
```

*NOTE:* An amplitude of 1.5 means that when comparing the average time of year to the peak, the peak is expected to be `exp(1.5)=4.5` times higher than average. We take the exponential because we have run a poisson regression (so think incident rate ratio).

**Question 2:** If seasonality exists, when are the high/low seasons?

### Yearly trend

**Question 3:** Is there a general yearly trend (i.e. increasing or decreasing from year to year?)

### Association With Rainfall

**Question 4:** Is daily rainfall associated with the number of cases?

## Autocorrelation

We check the `pacf` of the residuals to ensure that there is no autocorrelation. If we observe autocorrelation in our residuals, then we need to use a `robust` variance estimator (i.e. it makes our estimated variances bigger to account for our poor model fitting).

Here we see that our non-parametric seasonality model has not accounted for all of the associations in the data, so there is some autocorrelation in the residuals:

```{r}
d[, residuals := residuals(nfit1, type = "response")]
d[, predicted := predict(nfit1, type = "response")]
pacf(d$residuals)
```

Here we see that our parametric seasonality model has  accounted for all of the associations in the data, so there is no autocorrelation in the residuals:

```{r}
d[, residuals := residuals(pfit1, type = "response")]
d[, predicted := predict(pfit1, type = "response")]
pacf(d$residuals)
```

## Hints For Future Analyses

### Always Use A Denominator

```{r}
d <- data.table(
  year = c("1950", "2018"),
  Cases = c(350000, 530000),
  Population = c(3500000, 5300000)
)
d[, `Cases per\n100.000 Pop` := Cases / Population * 100000]
d <- melt.data.table(d, id.vars = "year")
```

We start out by considering the number of cases of a disease in 1950 and 2018. We see that the number of cases has increased dramatically over this time period!

```{r}
q <- ggplot(d[variable == "Cases"], aes(x = year, y = value, fill = variable))
q <- q + geom_col()
q <- q + scale_x_discrete("Year")
q <- q + scale_y_continuous("Number of people", labels = scales::format_format(
  big.mark = ".",
  decimal.mark = ",",
  scientific = FALSE
))
q <- q + scale_fill_brewer("", palette = "Set1")
q
```

However, upon taking the denominator (i.e. population) into consideration, we can see that the rate is consistent over time.

```{r}
q <- ggplot(d, aes(x = year, y = value, fill = variable))
q <- q + geom_col(position = "dodge")
q <- q + scale_x_discrete("")
q <- q + scale_y_continuous("", labels = scales::format_format(
  big.mark = ".",
  decimal.mark = ",",
  scientific = FALSE
))
q <- q + scale_fill_brewer("", palette = "Set1")
q <- q + facet_wrap(~variable, scales = "free")
q
```

### Negative Binomial Is Generally Better Than Poisson

Let us consider linear regression.

A linear regression model takes the form:

$$
y_i = \beta_0 + \beta_1 \times x_i + \text{error}_i
$$

Where

$$
\text{error}_i \sim N(0, \sigma)
$$
So basically, we have a straight line, and then the data is expected to be in a parallel band surrounding it:

```{r}
d <- data.table(x = runif(1000) * 10)
d[, y := 3 + 2 * x + rnorm(.N)]

fit <- lm(y ~ x, data = d)
resid_sd <- sd(fit$residuals)

thresholds <- data.table(x = c(0:10))
thresholds[, pred := predict(fit, newdata = thresholds)]
thresholds[, pred_l95 := pred - 1.96 * resid_sd]
thresholds[, pred_u95 := pred + 1.96 * resid_sd]

q <- ggplot(d, aes(x = x))
q <- q + geom_point(mapping = aes(y = y))
q <- q + geom_ribbon(data = thresholds, mapping = aes(ymin = pred_l95, ymax = pred_u95), alpha = 0.5)
q <- q + geom_line(data = thresholds, mapping = aes(y = pred), colour = "red", lwd = 2)
q
```

Poisson regression operates under the strong assumption that `mean=variance`. This means that when the mean is 150 (e.g. 150 cases per day), then the variance is also 150 (e.g. we expect between `r qpois(0.025,150)` and `r qpois(0.975,150)` cases each day). When the mean is 5 (e.g. 5 cases per day), then the variance is also 5 (e.g. we expect between `r qpois(0.025,5)` and `r qpois(0.975,5)` cases each day).

```{r}

d <- data.table(x = runif(1000) * 5)
d[, mu := exp(0.1 + x)]
d[, y := rpois(.N, mu)]

fit <- glm(y ~ x, data = d, family = "poisson")
resid_sd <- sd(fit$residuals)

thresholds <- data.table(x = c(0:50) / 10)
thresholds[, pred := predict(fit, newdata = thresholds, type = "response")]
thresholds[, pred_l95 := qpois(0.025, pred)]
thresholds[, pred_u95 := qpois(0.975, pred)]

q <- ggplot(d, aes(x = x))
q <- q + geom_point(mapping = aes(y = y))
q <- q + geom_ribbon(data = thresholds, mapping = aes(ymin = pred_l95, ymax = pred_u95), alpha = 0.5)
q <- q + geom_line(data = thresholds, mapping = aes(y = pred), colour = "red", lwd = 2)
q
```

This assumption may be true, or it may not be true. However, the main point is that it is a strong assumption, which makes the poisson regression less flexible.

A more flexible regression model for count data is the negative binomial model. Here, the "dispersion" (i.e. variance of the data) is estimated separately from the mean. You can think of it as being similar to a linear regression.

The benefits of the negative binomial regression model is that it is more flexible and is more likely to fit your data.

The downside of the negative binomial regression model is that it needs more data and may not converge. It is recommended to try and run a negative binomial regression model first, and then if it fails to converge, then run a poisson regression.

### Zeroes In Your Individial -> Aggregated Dataset

If your data is at the individual level (i.e. one row per case), then you will need to aggregate it to daily/weekly/monthly levels. If your data source is a registry, and you assume that your dataset contains all of the reported cases, then this means `"no reports"="no cases"`. This means that after aggregating, **you need to make sure that your collapsed/aggregated dataset has zeroes in it!!**

In the following dataset, we have one case per day from `2000-01-01` until `2000-01-29` and then again one case per day from `2000-08-08` until `2000-12-31`.

```{r}
d <- data.table(date = seq.Date(
  from = as.Date("2000-01-01"),
  to = as.Date("2000-12-31"),
  by = 1
))
d <- d[-c(30:220)]
d[, id := 1:.N]
d[, month := as.numeric(format.Date(date, "%m"))]

print(d)
```

If we collapse the data into months:

```{r}
collapsed <- d[, .(
  n = .N
), keyby = .(
  month
)]

print(collapsed)
```

You see that we are missing months `1` through to `6`. We cannot analyse this data, because **we do not have any zeros**.

How do we fix this? We create a `skeleton` of our results:

```{r}
skeleton <- data.table(month = 1:12)
print(skeleton)
```

We then merge our collapsed data with the skeleton:

```{r}
final <- merge(skeleton, collapsed, by = "month", all.x = TRUE)
print(final)
```

We then set all of the "missing" to 0:

```{r}
final[is.na(n), n := 0]
print(final)
```

Now we can analyze our data!

### Lagging Of Exposures

If you are interested in seeing how exposures affect your outcome (e.g. `rainfall`) then you might want to consider lagging your exposure. This will show you `how did the rainfall from last week affect the number of cases this week?`







